{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-community beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "% pip install -qU langchain-community beautifulsoup4\n",
    "%pip install -qU langchain-unstructured "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "page_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"\n",
    "\n",
    "loader = WebBaseLoader(web_paths=[page_url])\n",
    "docs = []\n",
    "async for doc in loader.alazy_load():\n",
    "    docs.append(doc)\n",
    "\n",
    "assert len(docs) == 1\n",
    "doc = docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'title': 'How to add memory to chatbots | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:', 'language': 'en'}\n",
      "\n",
      "How to add memory to chatbots | ü¶úÔ∏èüîó LangChain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG Applicatio\n"
     ]
    }
   ],
   "source": [
    "print(f\"{doc.metadata}\\n\")\n",
    "print(doc.page_content[:500].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=[page_url],\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(class_=\"theme-doc-markdown markdown\"),\n",
    "    },\n",
    "    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},\n",
    ")\n",
    "\n",
    "docs = []\n",
    "async for doc in loader.alazy_load():\n",
    "    docs.append(doc)\n",
    "\n",
    "assert len(docs) == 1\n",
    "doc = docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/'}\n",
      "\n",
      "How to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We'll go into more detail on a few techniques below! | note | This how-to guide previously built a chatbot using | RunnableWithMessageHistory | . You can access this version of the guide in the | v0.2 docs | . | As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of | LangGraph persistence | to incorporate | memory | into new LangChain applications. | If your code is already relying on | RunnableWithMessageHistory | or | BaseChatMessageHistory | , you do | not | need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses | RunnableWithMessageHistory | will continue to work as expected. | Please see | How to migrate to LangGraph Memory | for more details. | Setup | ​ | You'll need to install a few packages, and have your OpenAI API key set as an environment variable named | OPENAI_API_KEY | : | % | pip install | - | - | upgrade | - | - | quiet langchain langchain | - | openai langgraph | import | getpass | import | os | if | not | os | . | environ | . | get | ( | \"OPENAI_API_KEY\" | ) | : | os | . | environ | [ | \"OPENAI_API_KEY\" | ] | = | getpass | . | getpass | ( | \"OpenAI API Key:\" | ) | OpenAI API Key: ········ | Let's also set up a chat model that we'll use for the below examples. | from | langchain_openai | import | ChatOpenAI | model | = | ChatOpenAI | ( | model | = | \"gpt-4o-mini\" | ) | API Reference: | ChatOpenAI | Message passing | ​ | The simplest form of memory is simply passing chat history messages into a chain. Here's an example: | from | langchain_core | . | messages | import | AIMessage | , | HumanMessage | , | SystemMessage | from | langchain_core | . | prompts | import | ChatPromptTemplate | , | MessagesPlaceholder | prompt | = | ChatPromptTemplate | . | from_messages | ( | [ | SystemMessage | ( | content | = | \"You are a helpful assistant. Answer all questions to the best of your ability.\" | ) | , | MessagesPlaceholder | ( | variable_name | = | \"messages\" | ) | , | ] | ) | chain | = | prompt | | | model | ai_msg | = | chain | . | invoke | ( | { | \"messages\" | : | [ | HumanMessage | ( | content | = | \"Translate from English to French: I love programming.\" | ) | , | AIMessage | ( | content | = | \"J'adore la programmation.\" | ) | , | HumanMessage | ( | content | = | \"What did you just say?\" | ) | , | ] | , | } | ) | print | ( | ai_msg | . | content | ) | API Reference: | AIMessage | | | HumanMessage | | | SystemMessage | | | ChatPromptTemplate | | | MessagesPlaceholder | I said, \"I love programming\" in French: \"J'adore la programmation.\" | We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages. | Automatic history management | ​ | The previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's | persistence | . You can | enable persistence | in LangGraph applications by providing a | checkpointer | when compiling the graph. | from | langgraph | . | checkpoint | . | memory | import | MemorySaver | from | langgraph | . | graph | import | START | , | MessagesState | , | StateGraph | workflow | = | StateGraph | ( | state_schema | = | MessagesState | ) | # Define the function that calls the model | def | call_model | ( | state | : | MessagesState | ) | : | system_prompt | = | ( | \"You are a helpful assistant. \" | \"Answer all questions to the best of your ability.\" | ) | messages | = | [ | SystemMessage | ( | content | = | system_prompt | ) | ] | + | state | [ | \"messages\" | ] | response | = | model | . | invoke | ( | messages | ) | return | { | \"messages\" | : | response | } | # Define the node and edge | workflow | . | add_node | ( | \"model\" | , | call_model | ) | workflow | . | add_edge | ( | START | , | \"model\" | ) | # Add simple in-memory checkpointer | memory | = | MemorySaver | ( | ) | app | = | workflow | . | compile | ( | checkpointer | = | memory | ) | API Reference: | MemorySaver | | | StateGraph | We'll pass the latest input to the conversation here and let the LangGraph keep track of the conversation history using the checkpointer: | app | . | invoke | ( | { | \"messages\" | : | [ | HumanMessage | ( | content | = | \"Translate to French: I love programming.\" | ) | ] | } | , | config | = | { | \"configurable\" | : | { | \"thread_id\" | : | \"1\" | } | } | , | ) | {'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'), | AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]} | app | . | invoke | ( | { | \"messages\" | : | [ | HumanMessage | ( | content | = | \"What did I just ask you?\" | ) | ] | } | , | config | = | { | \"configurable\" | : | { | \"thread_id\" | : | \"1\" | } | } | , | ) | {'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'), | AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}), | HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'), | AIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]} | Modifying chat history | ​ | Modifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples: | Trimming messages | ​ | LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the | app | we declared above: | demo_ephemeral_chat_history | = | [ | HumanMessage | ( | content | = | \"Hey there! I'm Nemo.\" | ) | , | AIMessage | ( | content | = | \"Hello!\" | ) | , | HumanMessage | ( | content | = | \"How are you today?\" | ) | , | AIMessage | ( | content | = | \"Fine thanks!\" | ) | , | ] | app | . | invoke | ( | { | \"messages\" | : | demo_ephemeral_chat_history | + | [ | HumanMessage | ( | content | = | \"What's my name?\" | ) | ] | } | , | config | = | { | \"configurable\" | : | { | \"thread_id\" | : | \"2\" | } | } | , | ) | {'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'), | AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'), | HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'), | AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'), | HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'), | AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]} | We can see the app remembers the preloaded name. | But let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in | trim_messages | util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 \"token\" and keep only the last two messages: | from | langchain_core | . | messages | import | trim_messages | from | langgraph | . | checkpoint | . | memory | import | MemorySave\n"
     ]
    }
   ],
   "source": [
    "print(f\"{doc.metadata}\\n\")\n",
    "print(doc.page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]} | Note that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.\n"
     ]
    }
   ],
   "source": [
    "print(doc.page_content[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "page_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"\n",
    "loader = UnstructuredLoader(web_url=page_url)\n",
    "\n",
    "docs = []\n",
    "async for doc in loader.alazy_load():\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to add memory to chatbots\n",
      "A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\n",
      "Simply stuffing previous messages into a chat model prompt.\n",
      "The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n",
      "More complex modifications like synthesizing summaries for long running conversations.\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:5]:\n",
    "    print(doc.page_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How to add memory to chatbots\n",
      "NarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\n",
      "ListItem: Simply stuffing previous messages into a chat model prompt.\n",
      "ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n",
      "ListItem: More complex modifications like synthesizing summaries for long running conversations.\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:5]:\n",
    "    print(f'{doc.metadata[\"category\"]}: {doc.page_content}')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
